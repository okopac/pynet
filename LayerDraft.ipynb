{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sigmoid(x, der=False):\n",
    "    return sigmoid(x) * (1.0 - sigmoid(x)) if der else 1 / (1 + np.exp(-x))\n",
    "def simpleerror(actual, predicted):\n",
    "    return actual - predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NN():\n",
    "    def __init__(self, insize = 2, outsize = 1, layers=[3, 5], activation=sigmoid, error_func=simpleerror, learning_rate = 0.3):\n",
    "        self.activation = activation\n",
    "        self.layers = [insize] + layers + [outsize]\n",
    "        self.params = []\n",
    "        self.learning_rate = learning_rate\n",
    "        self.error_func = error_func\n",
    "        np.random.seed(0)\n",
    "        self.values = []\n",
    "        #\n",
    "        #  x     x y C     1a\n",
    "        #  y  .  x y C  =  1b\n",
    "        #        x y C     1c\n",
    "        #\n",
    "        #  1a    1a 1b 1c CC\n",
    "        #  1b .  \n",
    "        #  1c\n",
    "        #\n",
    "        for i in range(1, len(self.layers)):\n",
    "            # Each transformation takes the vector of size previous layer to the next layer size.\n",
    "            # However we also need to add a constant at each layer\n",
    "            # Rather than doing this seperately, if we extend the previous layer by 1 element (with the value 1)\n",
    "            # Then we can perform a matrix multiplication to map to the next layer size\n",
    "            self.params.append(np.random.random((self.layers[i - 1] + 1, self.layers[i])) - 0.5)\n",
    "            \n",
    "#             if i == len(self.layers) - 1:\n",
    "#                 # The last layer has no bias parameter\n",
    "#                 self.params.append(np.random.random((self.layers[i - 1] + 1 , self.layers[i])) - 0.5)\n",
    "#             else:\n",
    "#                 # Each layer maps from the previous layers parameters (plue one for bias) to the next ()\n",
    "#                 self.params.append(np.random.random((self.layers[i - 1] + 1, self.layers[i] + 1)) - 0.5)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # We build an array of the values at each layer\n",
    "        # This will be the same size as the layers\n",
    "        # Start with the input\n",
    "        self.values = [np.array(x)]\n",
    "        for i, p in enumerate(self.params):\n",
    "            # Create the next layer by dot product with the parameter matrix\n",
    "            # Note we add the constant 1, as the parameter matrix contains a constant\n",
    "            output = np.dot(np.append(self.values[i], 1), p)\n",
    "            # Run the activation function to map these values\n",
    "            act = self.activation(output)\n",
    "            # Then add to the runnning values total\n",
    "            self.values.append(act)\n",
    "            \n",
    "        # We return the output, which is the last layer\n",
    "        return self.values[-1]\n",
    "        \n",
    "    def train(self, x, y):\n",
    "        # The input vector must be the same size as the first matrix (minus the constant)\n",
    "        assert(len(x) == self.params[0].shape[0] - 1)\n",
    "        # The output sizes should match\n",
    "        assert(len(y) == self.params[-1].shape[1])\n",
    "        \n",
    "        # Get the output\n",
    "        res = self.forward(x)\n",
    "        \n",
    "        # Calculate the error\n",
    "        error = self.error_func(y, self.values[-1])\n",
    "        \n",
    "        # Start building the errors\n",
    "        # We start by taking the error, and back propagating through the activation function\n",
    "        self.errors = [error * self.activation(self.values[-1], True)]\n",
    "\n",
    "        # Then we \n",
    "        last = True\n",
    "        for l in range(len(self.values) - 2, -1, -1):\n",
    "            # Take the previous error, and backprop through the parameter matrix\n",
    "            # We take the transverse matrix here to invert the operation\n",
    "            # We then drop the last value, as this is the 1 that we appending in the forward model\n",
    "            if l == len(self.values) - 2:\n",
    "                back_prop_errors = self.errors[-1].dot(self.params[l].T)\n",
    "            else:\n",
    "                back_prop_errors = self.errors[-1][:-1].dot(self.params[l].T)\n",
    "            \n",
    "            # Apply the activation function deriviative to pass back to the next layer\n",
    "            # Remember the implicit 1 value at each layer\n",
    "            #self.errors.append(back_prop_errors * self.activation(self.values[l], True))\n",
    "            self.errors.append(back_prop_errors * np.append(self.activation(self.values[l], True),1))\n",
    "            \n",
    "        self.errors.reverse()\n",
    "        print('errors %s' % [e.shape for e in self.errors])\n",
    "        for i in range(len(self.params)):\n",
    "            layer = np.atleast_2d(np.append(self.values[i], 1))\n",
    "            error = np.atleast_2d(self.errors[i])\n",
    "            print(layer)\n",
    "            print(error)\n",
    "            print(i)\n",
    "            print(layer.shape)\n",
    "            print(error.shape)\n",
    "            print(layer.T.dot(error).shape)\n",
    "            print(self.params[i].shape)\n",
    "            self.params[i] += self.learning_rate * layer.T.dot(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params [(3, 3), (4, 5), (6, 1)]\n",
      "vakues [(2,), (3,), (5,), (1,)]\n",
      "errors [(3,), (4,), (6,), (1,)]\n",
      "[[2 2 1]]\n",
      "[[ -3.28829145e-06  -4.77233593e-06  -1.02023150e-04]]\n",
      "0\n",
      "(1, 3)\n",
      "(1, 3)\n",
      "(3, 3)\n",
      "(3, 3)\n",
      "[[ 0.53120454  0.66138253  0.72331752  1.        ]]\n",
      "[[ -1.66057744e-04   1.32106907e-05  -2.53552535e-04  -3.49186181e-03]]\n",
      "1\n",
      "(1, 4)\n",
      "(1, 4)\n",
      "(4, 4)\n",
      "(4, 5)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4,5) (4,4) (4,5) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-403-35743a0532cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"params %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vakues %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-402-e40adbf4c4d8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,5) (4,4) (4,5) "
     ]
    }
   ],
   "source": [
    "nn = NN()\n",
    "nn.forward([2, 2])\n",
    "print(\"params %s\" % [p.shape for p in nn.params])\n",
    "print(\"vakues %s\" % [p.shape for p in nn.values])\n",
    "nn.train([2, 2], [0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 3), (4, 5), (6, 1)]\n",
      "[2, 3, 5, 1]\n",
      "[ 0.3704215]\n",
      "[(2,), (3,), (5,), (1,)]\n",
      "---\n",
      "[2, 3, 5, 1]\n",
      "[ 0.3704215]\n",
      "[(2,), (3,), (5,), (1,)]\n"
     ]
    }
   ],
   "source": [
    "nn = NN()\n",
    "print([p.shape for p in nn.params])\n",
    "print(nn.layers)\n",
    "print(nn.forward(np.array([2, 2])))\n",
    "print([p.shape for p in nn.values])\n",
    "\n",
    "print('---')\n",
    "print(nn.layers)\n",
    "print(nn.forward(np.array([2, 2])))\n",
    "print([p.shape for p in nn.values])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h', ' ', 'l', 'o', 'e']"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letters = 'hello hello hello'\n",
    "possible = list(set(letters))\n",
    "possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzqCXpI4z6CWp4/4/QN/U16akw28AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1153e2f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data, labels = make_blobs()\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "nn = NN(outsize=3)\n",
    "\n",
    "def accuracy(nn, data, label):\n",
    "    return sum([np.argmax(nn.forward(d)) == l for d, l in zip(data, label)]) * 100. / len(data)\n",
    "\n",
    "err = []\n",
    "for i in range(5000):\n",
    "    i = random.randint(0, len(data) - 1)\n",
    "    lprob = np.array([1. if j == labels[i] else 0. for j in range(3)])\n",
    "    nn.train(data[i], lprob)\n",
    "    err.append(accuracy(nn, data, labels))\n",
    "plt.plot(err)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    out = []\n",
    "    for j in range(50):\n",
    "        out.append(np.argmax(nn.forward(np.array([i*20/50. - 10., j*20./50. - 10.]))))\n",
    "    print(' '.join((\"%s\" % i for i in out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -6.39862121,  -7.55846418],\n",
       "       [ -5.94496305,  -4.76765649],\n",
       "       [ -5.71211416,  -4.20477745],\n",
       "       [ -0.88524313,   6.05577782],\n",
       "       [ -6.26274226,  -5.86106999],\n",
       "       [ -8.41022279,   7.57836476],\n",
       "       [ -3.17515249,   6.94099034],\n",
       "       [ -6.6317687 ,  -7.811411  ],\n",
       "       [ -8.20040433,  -8.05913159],\n",
       "       [ -7.58996883,  -6.80910024],\n",
       "       [ -7.82939247,   6.50123439],\n",
       "       [ -9.14700892,   7.74301544],\n",
       "       [ -7.6547888 ,   6.0748389 ],\n",
       "       [ -3.55365577,   7.91058651],\n",
       "       [ -2.43263572,   5.99899978],\n",
       "       [ -2.42999725,   5.94826026],\n",
       "       [ -6.20552503,  -7.34534175],\n",
       "       [ -4.38753547,   6.13124138],\n",
       "       [ -2.20108413,   7.50917364],\n",
       "       [ -8.53023289,   5.24553711],\n",
       "       [ -8.68187655,  -5.69119216],\n",
       "       [ -2.54278665,   5.12880614],\n",
       "       [ -7.60067528,  -6.74713016],\n",
       "       [ -8.75736931,   7.67260396],\n",
       "       [ -5.76035114,  -5.42157267],\n",
       "       [ -3.40284324,   7.53580305],\n",
       "       [ -6.29878213,  -6.32296086],\n",
       "       [ -2.26506932,   6.82801272],\n",
       "       [ -8.00760377,   5.37868727],\n",
       "       [ -7.82215616,  -5.76958073],\n",
       "       [ -2.5311623 ,   4.93396074],\n",
       "       [ -9.05554202,   5.95885994],\n",
       "       [ -9.40113121,   8.18440428],\n",
       "       [ -7.06534053,  -7.79402412],\n",
       "       [ -0.81314194,   4.12572712],\n",
       "       [ -8.31964655,   6.57951846],\n",
       "       [ -2.51714704,   6.68153307],\n",
       "       [ -6.89475358,  -7.31923898],\n",
       "       [ -8.22724426,  -5.05047272],\n",
       "       [ -8.75074506,   7.89672502],\n",
       "       [ -6.76419198,  -5.24843547],\n",
       "       [ -7.53598675,   7.95615801],\n",
       "       [ -1.06139401,   7.03096831],\n",
       "       [ -8.4404607 ,   7.45056007],\n",
       "       [ -3.54025192,   7.27810238],\n",
       "       [ -6.31109022,  -6.97313892],\n",
       "       [ -8.22650112,  -4.19595869],\n",
       "       [ -3.50285991,   4.7987771 ],\n",
       "       [ -7.63107846,   6.74604876],\n",
       "       [ -8.07467194,  -5.68718902],\n",
       "       [ -9.21718081,   6.32236106],\n",
       "       [ -2.40323669,   6.85504756],\n",
       "       [ -7.47741456,  -6.01500786],\n",
       "       [ -4.52608085,   5.2822854 ],\n",
       "       [ -7.79401418,   6.12925624],\n",
       "       [ -5.39264944,  -4.54284553],\n",
       "       [ -7.66566643,   7.00053296],\n",
       "       [ -3.20716958,   5.88203064],\n",
       "       [ -7.85089891,   7.28236118],\n",
       "       [ -1.28154217,   5.60164364],\n",
       "       [ -3.56102704,   6.70725225],\n",
       "       [ -1.97428724,   5.28936815],\n",
       "       [ -7.3068982 ,  -5.46419306],\n",
       "       [ -6.7956523 ,   4.75334156],\n",
       "       [ -6.13662753,   6.29911763],\n",
       "       [ -6.136699  ,  -5.12845579],\n",
       "       [ -1.57751436,   6.66982214],\n",
       "       [ -2.67124469,   6.46976301],\n",
       "       [ -6.32882136,  -6.65417564],\n",
       "       [ -8.22713174,   5.99498448],\n",
       "       [ -7.9113628 ,  -6.20113702],\n",
       "       [ -1.42817461,   8.30321845],\n",
       "       [ -4.18152026,   6.36539975],\n",
       "       [ -1.28604357,   4.65243095],\n",
       "       [ -3.00968227,   7.05206601],\n",
       "       [ -2.42785395,   6.83376457],\n",
       "       [ -8.39000488,   6.46279775],\n",
       "       [ -7.14661025,  -5.95349889],\n",
       "       [ -8.73075927,   6.89222265],\n",
       "       [ -8.30655598,  -6.27748138],\n",
       "       [ -7.56521509,   6.84901094],\n",
       "       [ -4.56621226,   7.86470031],\n",
       "       [ -4.43677538,   5.78868938],\n",
       "       [ -6.95034981,   6.9351369 ],\n",
       "       [ -1.91127277,   8.70419795],\n",
       "       [ -3.28660082,   5.23967358],\n",
       "       [ -7.44276192,  -5.5799263 ],\n",
       "       [ -6.62995197,  -6.06659193],\n",
       "       [ -7.07568243,  -6.92733809],\n",
       "       [ -9.1207694 ,   5.87249188],\n",
       "       [ -7.95403857,   5.92932696],\n",
       "       [ -6.31813815,  -6.22715324],\n",
       "       [ -8.55604759,   6.39600704],\n",
       "       [ -6.61653991,  -4.27718431],\n",
       "       [ -6.81094757,  -6.09632395],\n",
       "       [ -8.99832803,   7.68849248],\n",
       "       [ -7.70408947,   7.80648349],\n",
       "       [-10.27633524,   6.57539527],\n",
       "       [ -8.48187072,   3.24413003],\n",
       "       [ -8.01107002,   5.72665511]])"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
